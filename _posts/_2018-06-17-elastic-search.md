---
title: Полнотекстовый поиск с Elastic Search
header:
  teaser: images/elastic-search/search_example.png
excerpt: Несколько рецептов приготовления
date: '2018-06-17 00:00:00'
---

В проекте, которым я занимаюсь в [Контуре](https://kontur.ru/), мы решили сделать полнотекстовый поиск по основным сущностям системы. Мы не изобрели велосипед — взяли [Elastic Search](https://www.elastic.co/products/elasticsearch) и дотюнили до наших нужд. Elastic Search дает богатые возможности для полнотекстового поиска, предоставляет шардирование и репликацию данных, в общем — классный инструмент. Логика поиска с использованием Elastic умещается всего в несколько сотен строк кода. Код компактный, но за ним спрятано парочка хаков и понимание устройства Elastic Search. В статье хочу рассказать о фишках в организации поиска.

## Что ищем

Мой проект — это внутренний биллинг компании (Контур.Биллинг), один из пользователей — продавцы. Продавцу нужно быстро найти клиента в нашей системе по любой информации, которая у него есть — ФИО, номер телефона, email, ИНН, номер заказа. Для продавца поиск выглядит примерно так:

![Пример поискового запроса](/images/elastic-search/search_example.png 'Пример поискового запроса'){: .align-center}

Строка поиска — обычный autocomplete. Пока пользователь набирает запрос, всплывают подсказки с найденными клиентами.

Кроме записей о клиенте, в поиске ищут заявки по работе с клиентом, заказы, контакты, счета, юридические документы и прочее.

Для каждой сущности системы мы храним документ из нескольких полей:
  - Текст, по которому можно найти документ;
  - Мета-информация о документе — например, тип документа;
  - Список пользователей, которые могут видеть документ.

Текст по-разному формируется для разных сущностей — по клиенту это ИНН-КПП клиента и название организации (Контур работает в B2B, поэтому большинство клиентов идентифицируются по реквизитам), для заявки — ФИО клиента и телефон, который он оставил в заявке, и так далее.

## Индексирование данных

Данные в Биллинге хранятся в нескольких базах данных, в основном — в&nbsp;Microsoft SQL и Apache Cassandra. Есть индексирующий процесс, который просыпается по [расписанию](https://ru.wikipedia.org/wiki/Cron), вычитывает изменившиеся данные из базы, отправляет их в Elastic. Elastic хранит лишь копию данных, необходимых для поиска.

В чем плюсы такого подхода:
  - В отличие от синхронной записи (записали в БД — сразу записали в Elastic) получаем дополнительную отказоустойчивость. Бывает так, что Elastic тупит и не может записать данные ([долго собирает мусор](https://ru.wikipedia.org/wiki/%D0%A1%D0%B1%D0%BE%D1%80%D0%BA%D0%B0_%D0%BC%D1%83%D1%81%D0%BE%D1%80%D0%B0), [тупанула сеть](https://aphyr.com/posts/288-the-network-is-reliable)). Что делать при синхронной записи неясно — данные уже есть в БД, а в Elastic нет, транзакционно записать в Elastic нельзя. Асинхронный процесс гарантирует [eventual consistency](https://en.wikipedia.org/wiki/Eventual_consistency) — данные в конечном счете окажутся в Elastic. Если не получилось записать сразу, то процесс повторит попытку позже;
  - Elastic не используется как первичное хранилище. Данные можно безболезненно потерять и пересобрать индекс заново. Пару раз это здорово выручало меня, когда делал изменение схемы данных — я забил на поддержку обратной совместимости, создал новый индекс, накачал его данными и переключил пользователей на чтение из нового индекса.

В чем минусы:
  - Есть задержка на появление данных в поиске, поскольку индексирующий процесс работает по расписанию. Задержку можно уменьшать, настраивая время запуска процесса;
  - Конкретно в нашей однопоточной схеме — индексация слишком медленная, если данных много. В Биллинге небольшой индекс на десятки гигабайт и несколько десятков миллионов документов. Его индексация занимает 10-12 часов. Не слишком быстро — но пока нас устраивает.

Есть прикольный альтернативный подход к индексации с помощью очереди сообщений — записываем все события по изменению сущностей в очередь [Kafka](http://kafka.apache.org/), а потом несколько индексирующих процессов разгребают очередь сообщений и индексируют документы в Elastic. Подход с очередью лучше масштабируется. Пример индексации с помощью очереди можно посмотреть на [Github](https://github.com/BigDataDevs/kafka-elasticsearch-consumer).

## Анализ текста

Основная структура данных в Elastic — это [инвертированный индекс](https://ru.wikipedia.org/wiki/%D0%98%D0%BD%D0%B2%D0%B5%D1%80%D1%82%D0%B8%D1%80%D0%BE%D0%B2%D0%B0%D0%BD%D0%BD%D1%8B%D0%B9_%D0%B8%D0%BD%D0%B4%D0%B5%D0%BA%D1%81). Это индекс, в котором для каждого слова хранится, в каких документах оно встречается.

![Инвертированный индекс](/images/elastic-search/inverted-index.svg 'Инвертированный индекс'){: .align-center}

Такой индекс эффективен, когда мы ищем документы с вхождением слова. Если нужно найти вхождения комбинации слов, то можно взять списки для каждого слова и пересечь их.

Чтобы построить такой индекс, Elastic прогоняет текст через несколько шагов: 

![Этапы анализа](/images/elastic-search/analysis.png 'Этапы анализа'){: .align-center}

  1. CharFilter — фильтрация входных данных. Здесь отбрасываются символы, которые не несут полезной информации для поиска, например, служебные символы, html-верстка.
  2. Tokenizer — токенизация или разбиение текста на слова. 
  3. TokenFilter — преобразование полученых слов. Например, каждое слово можно привести к нижнему регистру или заменить на слово-синоним. Можно вообще выкинуть слово из индекса, например, если это нецензурное слово.

## Поиск

Поисковый запрос в Elastic состоит из двух частей — [Filter и Query](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-filter-context.html).
  - Filter — отвечает на вопрос "попадает ли документ под условия поиска";
  - Query — "насколько хорошо документ подходит под условия поиска".

Отличие Query в том, что кроме формальной проверки "подходит" — "не подходит", вычисляется еще и [релевантность](https://www.elastic.co/guide/en/elasticsearch/guide/current/scoring-theory.html) подходящего документа. Все найденные документы затем ранжируются по релевантности. [Формула релевантности](https://www.elastic.co/guide/en/elasticsearch/guide/current/practical-scoring-function.html) — хитрый матанализ, но коротко поведение функции описывается так:
  - Релевантнее те документы, где больше вхождений искомых слов;
  - Менее релевантны те документы, где встречаются самые популярные слова в индексе. Например, союзы, предлоги, вводные слова встречаются во всех текстах — и слабо влияют на релевантность документа;
  - Короткие тексты более релевантны (вероятность встретить искомое слово в коротком тексте меньше, чем в длинном).

Самые простые способы найти что-то — запросы match и phrase.

[Match](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query.html) запрос — принимает на вход текст запроса, анализирует его, ищет документы со словами из текста. Например, если хотим найти все слова из запроса, то подойдет такой запрос:

```json
{
    "query": {
        "match" : {
            "message" : {
                "query" : "все слова должны встретиться в документе",
                "operator" : "and"
            }
        }
    }
}
```

[Match Phrase](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase.html) — то же самое, что Match, но требует от документа, чтобы слова встречались в правильном порядке. Не все слова обязаны идти строго друг за другом в тексте, можно настроить число слов, которые разделяют два искомых слова в фразе с помощью параметра `slop`.

```json
{
    "query": {
        "match_phrase" : {
            "message" : "ищем точное вхождение фразы в тексте",
            "slop": 0
        }
    }
}
```

[Match Phrase Prefix](https://www.elastic.co/guide/en/elasticsearch/reference/current/query-dsl-match-query-phrase-prefix.html) — в отличие от Match Phrase, у последнего слова ищем совпадение префикса.

```json
{
    "query": {
        "match_phrase_prefix" : {
            "message" : {
                "query" : "ищем вхожде",
                "max_expansions" : 10
            }
        }
    }
}
```

Для простого autocomplete подходит как раз match_phrase_prefix. Однако, этот запрос стоит использоваться с осторожностью — он работает недетерменировано, поскольку выбирает лишь `max_expansions` слов в индексе, которые начинаются с вхождения префикса (в нашем запросе — `вхожде`), а потом ищет документы с такими словами. При слишком маленьком max_expansions пользователь не найден нужный документ, при слишком большом поиск будет работать медленно.

## Авторизация запроса

Биллинг хранит чувствительные данные компании. Поэтому любой запрос пользователя авторизуется. Для авторизации доступа к документам в поиске мы используем паттерн Access Control List.

[Access Control List](https://ru.wikipedia.org/wiki/ACL) (ACL) — паттерн для избирательного предоставления доступа к документу. В документе мы сохраняем список пользователей, которым доступен этот документ. В Биллинге размер ACL ограничен десятком пользователей, поэтому документ получается не слишком пухлый. Авторизация по ACL делается так — к любому запросу пользователя в Elastic добавляется запрос по вложеному документу [(Nested query)](https://www.elastic.co/guide/en/elasticsearch/reference/6.0//query-dsl-nested-query.html).

Пример фильтра:
```json
{
  "filter": [{
    "nested": {
      "path": "accessControlList",
      "query": {
        "bool": {
          "filter": [{
            "term": {
              "accessControlList.userId": {
                "value": "d32c608c4d484a058bcf759e3c68eb28"
              }
            }
          }]
        }
      }
    }
  }]
}
```

В Nested-фильтре указан путь `path` до вложенного документа. Во вложенном запросе фильтруем по `userId` — идентификатору пользователя.

## "Объясни" — API 

Для неискушенного инженера поиск в Elastic работает как магия. Иногда категорически непонятно, почему документ подошел под критерии поиска. Мне кажется, я потратил человеко-дни на медитацию над некоторыми запросами, когда только начинал изучать Elastic.

Чтобы понимать работу Elastic, не нужно разбираться в его исходниках. Разработчики дали два удобных API:
  - [Analyze API](https://www.elastic.co/guide/en/elasticsearch/reference/current/indices-analyze.html) — прогоняет текст через указанный анализатор и показывает, какие слова Elastic сохранит в индекс.
  - [Explain API](https://www.elastic.co/guide/en/elasticsearch/reference/6.3/search-explain.html) - объясняет, почему документ подходит или не подходит под критерии поиска, показывает релевантность документа и как она вычислена. 

Я использую эти API для отладки:
  - Если настраиваю свой анализатор и хочу проверить, как она работает (особенно если где-то фигурируют регулярные выражения);
  - Когда поиск не находит нужный документ или находит лишний;
  - Когда находятся правильные документы, но более релевантные оказываются в выдаче ниже менее релевантных. Тогда лезу в&nbsp;Explain&nbsp;API и зарываюсь в формулу расчета релевантности.

## Тюним точность

Ngram'ы - для поиска по подстроке

Поиск по телефонам

Ё-е

Переключение раскладки за пользователя

## Проблема с релевантностью

Упорядочиваем по типу сущности

Альтернатива - группировка на UI

Когда boost не работает. Ракитянская

Constant_score - более простое решение здесь и сейчас, не хотели переделывать интерфейс.

---

TF-IDF - мера для ранжирования документов. Пример - допустим мы собрали архив статей о путешествиях в разные страны (блоги, репортажи, гайды и т.д.) и хотим сделать поисковую строку в стиле Google. Пользователь вводит запрос "винные дороги Кахетии". Как определить, какая статья больше подходит под запрос пользователя?

TF-IDF считает рейтинг каждого документа, где встречаются такие слова. Рейтинг считается как произведение двух величин:
  - TF (termin frequency) - то, как часто слово встречается в документе (в сравнении с общим числом слов)
  - IDF (inverse document frequency) - то, насколько уникально слово в коллекции документов. Чем уникальнее и реже, тем лучше.
TF-IDF будет поднимать в выдаче документы, в которых часто встречается слово "Кахетия" - оно встречается редко и, скорей всего, в таких документах есть то, что нужно. Слово "дороги" будет меньше влиять на рейтинг документа - так как в статьях о путешествиях встречается широко, а значит и плохо помогает отличить статьи о винных дорогах Кахетии от статей об автостопе по Франции.

В Эластике используется модификация TF-IDF.
tf(t in d) = √frequency (square root)
idf(t) = 1 + log ( numDocs / (docFreq + 1)) // docFreq - как часто встречается в документах
norm(d) = 1 / √numTerms (отключаемая штука)

В каких пределах исчисляется эта штука?
- tf хорошо бы в 1 (можно оставлять уникальный термы). Плюс у нас индекс такой, что там редко бывает что-то сильно больше
- idf
  - Для плательщиков - 1 + log (30 млн / (5 млн + 1)) = 1 + log 6 ~ 3.5
  - Для счетов - 1 + log (30 млн / (15 млн + 1)) = 1 + log 2 = 2
  - Для ПП - 1 + log (30 млн / 3 млн) = 1 + log 10 ~ 4.1
  - Короче, чем реже, тем лучше
- norm
  - для счетов - терм всего один + анализируется. 
  - для плательщиков - инн/кпп + название.
  - для ПП - инн/кпп + название + длинный список контактов. 
Взять примеры tf-idf с боевой, показать, что коэффициент norm очень сильно перевешивает 

Для запроса из одного слова посчитать TF-IDF довольно легко. Как посчитать, когда запрос состоит из нескольких слов: 

score(q,d)  = queryNorm(q) · coord(q,d) · ∑ ( tf(t in d) · idf(t)² · t.getBoost() · norm(t,d) ) (t in q)    

queryNorm - не зависит от документа, нужен только для сравнения результатов разных запросов друг с другом. Про него говорить не будем.

coord(q,d) = number of matching terms(q,d) / total terms (q)
Когда в should несколько слов, то поднимает выше те, где больше слов встретилось. Тоже забьем на него

t.getBoost() - вес, который мы прописали терму

Видим, что getBoost - это какая-то константа, а norm отличается разительно в зависимости от данных. 

Короче, это плохая затея делать boost по типам сущностей - для них проще задать предсказуемый constantScore. И еще добавлять constantScore, если совпало точно. 

Если сущности совпадают - окей, тогда давайте прибавим score. 

